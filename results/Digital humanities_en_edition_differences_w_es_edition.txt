How to read this document: Passages from one language's article appear in order of their likelihood of containing different content from the other language. The further down you read, the more likely that the content between languages is similar. If the list is short, it is likely that the content between languages is highly similar, or that the original language article for this subject is not very long. Passages may be from any part of an article.  Sometimes they are captions for pictures, sometimes references, and sometimes they are part of the article itself. Not every passage will be meaningful.  This is merely a guide to help you find the most novel content, quickly.

Digital humanities

en edition's differences with the es version: 


1. 
| url = http://www.mcg.uva.nl/abstracts/honing-2007b.html | journal = [[International Journal of Humanities and Arts Computing]] | volume = 1 | issue = 1| pages = 67–75 | doi = 10.3366/E1753854808000104 }}
* Inman James, Reed, Cheryl, & Sands, Peter, eds. 

2. 
Some are continually updated while others may not be due to loss of support or interest, though they may still remain online in either a [[beta version]] or a finished form. 

3. 
An accessible, free example of an online textual analysis program is [[Voyant Tools]],<ref>{{cite web|url=http://voyant-tools.org/|title=Voyant Tools|website=voyant-tools.org}}</ref> which only requires the user to copy and paste either a body of text or a URL and then click the 'reveal' button to run the program. 

4. 
| year = 2010 | title = The Structure of EU Mediasphere | journal = PLoS ONE | volume = 5 | issue = 12| page = e14243 | doi=10.1371/journal.pone.0014243 | pmid=21170383 | pmc=2999531}}</ref><ref>{{cite journal | last1 = Lampos | first1 = V | last2 = Cristianini | first2 = N | year = 2012| title = Nowcasting Events from the Social Web with Statistical Learning | doi = 10.1145/2337542.2337557 | journal = ACM Transactions on Intelligent Systems and Technology  | volume = 3 | issue = 4| page = 72 }}</ref><ref>NOAM: news outlets analysis and monitoring system; I Flaounas, O Ali, M Turchi, T Snowsill, F Nicart, T De Bie, N Cristianini Proc. 

5. 
McGann | title = Rossetti Archive| accessdate = 2012-06-16| date =| url = http://www.rossettiarchive.org/}}</ref> and ''[[The William Blake Archive]]''<ref>{{Citation| publisher = | editors = Morris Eaves, Robert Essick, and Joseph Viscomi| title = The William Blake Archive| accessdate = 2012-06-16| date =| url = http://www.blakearchive.org/}}</ref>), which demonstrated the sophistication and robustness of text-encoding for literature.<ref>{{Cite journal| issn = 0093-1896| volume = 31| issue = 1| pages = 49–84| last = Liu| first = Alan| title = Transcendental Data: Toward a Cultural History and Aesthetics of the New Encoded Discourse| journal = Critical Inquiry| year = 2004| jstor = 10.1086/427302| doi=10.1086/427302| url = http://www.escholarship.org/uc/item/45m8t3xb}}</ref> The advent of personal computing and the World Wide Web meant that Digital Humanities work could become less centered on text and more on design. 

6. 
| last=Letcher | title=Cultoromics: A New Way to See Temporal Changes in the Prevalence of Words and Phrases | journal=American Institute of Higher Education 6th International Conference Proceedings | volume=4 | issue=1 | page=228 | date=April 6, 2011 | access-date=January 16, 2017 | archive-url=https://web.archive.org/web/20160303215026/http://www.amhighed.com/documents/charleston2011/AIHE2011_Proceedings.pdf#page=228 | archive-date=March 3, 2016 | url-status=dead }}</ref> The term is an American [[neologism]] first described in a 2010 ''[[Science (journal)|Science]]'' article called ''Quantitative Analysis of Culture Using Millions of Digitized Books'', co-authored by Harvard researchers Jean-Baptiste Michel and [[Erez Lieberman Aiden]].<ref name="Jean">{{cite journal | first1=Jean-Baptiste | last1=Michel | first2=Erez | last2=Liberman Aiden | title=Quantitative Analysis of Culture Using Millions of Digitized Books | journal=[[Science (journal)|Science]] | date=16 December 2010 | doi=10.1126/science.1199644 | pmid=21163965 | volume=331 | issue=6014 | pmc=3279742 | pages=176–82}}</ref>

A 2017 study<ref name="Lansdall-Welfare 201606380"/> published in the [[Proceedings of the National Academy of Sciences of the United States of America]] compared the trajectory of n-grams over time in both digitised books from the 2010 [[Science (journal)|Science]] article<ref name="Jean" /> with those found in a large corpus of regional newspapers from the United Kingdom over the course of 150 years. 

7. 
The following are a few examples of the variety of projects in the field:<ref>See [https://wiki.commons.gc.cuny.edu/Sample_Projects/ CUNY Academic Commons Wiki Archive] for more.</ref>

===Digital archives===

The [[Women Writers Project]] (begun in 1988) is a long-term research project to make pre-Victorian women writers more accessible through an electronic collection of rare texts. 

8. 
These range from research developed by organizations such as [[SIGGRAPH]] to creations by artists such as [[Charles and Ray Eames]] and the members of [[Experiments in Art and Technology|E.A.T.]] (Experiments in Art and Technology). 

9. 
TAPoR (Text Analysis Portal for Research<ref>{{cite web|url=http://tapor-test.artsrn.ualberta.ca/home|archive-url=https://archive.is/20170929202656/http://tapor-test.artsrn.ualberta.ca/home|url-status=dead|archive-date=2017-09-29|title=TAPoR|website=tapor-test.artsrn.ualberta.ca}}</ref>) is a gateway to text analysis and retrieval tools. 

10. 
These include the digitization of 17th-century manuscripts, an electronic corpus of Mexican history from the 16th to 19th century, and the visualization of pre-Hispanic archaeological sites in [[3D computer graphics|3-D]].<ref>{{Cite book|title=Digital Humanities in Practice|last=Warwick|first=Claire|last2=Terras|first2=Melissa|last3=Nyhan|publisher=Facet Publishing|year=2012|isbn=9781856047661|location=London|pages=203}}</ref>

===Cultural analytics===

"Cultural analytics" refers to the use of computational method for exploration and analysis of large visual collections and also contemporary digital media. 

11. 
The lab has been using methods from the field of computer science called Computer Vision many types of both historical and contemporary visual media—for example, all covers of ''Time'' magazine published between 1923 and 2009,<ref>{{Cite web|url=http://lab.culturalanalytics.info/2016/04/timeline-4535-time-magazine-covers-1923.html|title=Timeline: 4535 Time Magazine Covers, 1923-2009|last=|first=|date=|website=|access-date=}}</ref> 20,000 historical art photographs from the collection in Museum of Modern Art (MoMA) in New York,<ref>{{Cite web|url=http://lab.culturalanalytics.info/2016/04/exploratory-visualizations-of-thomas.html|title=A View from Above: Exploratory Visualizations of MoMA Photography Collection|last=|first=|authorlink=Nadav Hochman and Lev Manovich|date=|website=|access-date=}}</ref> one million pages from Manga books,<ref>{{Cite web|url=http://lab.culturalanalytics.info/2010/11/one-million-manga-pages_14.html|title=Exploring One Million Manga Pages with Supercomputers and HIPerSpace|last=|first=|date=|website=|access-date=}}</ref> and 16 million images shared on Instagram in 17 global cities.<ref>{{Cite book|url=http://manovich.net/index.php/projects/instagram-and-contemporary-image|title=Instagram and Contemporary Image|last=Manovich|first=Lev|publisher=|year=2017|isbn=|location=|pages=}}</ref> Cultural analytics also includes using methods from media design and data visualization to create interactive visual interfaces for exploration of large visual collections e.g., Selfiecity and On Broadway. 

12. 
''Electronic Text in the Humanities: Principles and Practice'', Oxford: Oxford University Press. 

13. 
83.</ref> Some scholars use advanced programming languages and databases, while others use less complex tools, depending on their needs. 

14. 
Here we might also reflect on the way in which the practice of making-visible also entails the making-invisible – computation involves making choices about what is to be captured. 

15. 
''[http://users.ox.ac.uk/~ctitext2/resguide2000/contents.shtml Oxford University Computing Services Guide to Digital Resources for the Humanities]'', West Virginia University Press. 

16. 

Digital humanities scholars use computational methods either to answer existing research questions or to challenge existing theoretical paradigms, generating new questions and pioneering new approaches. 

17. 
The analysis of vast quantities of historical newspaper content has showed how periodic structures can be automatically discovered, and a similar analysis was performed on social media.<ref>{{Cite journal|last=Dzogang|first=Fabon|last2=Lansdall-Welfare|first2=Thomas|last3=Team|first3=FindMyPast Newspaper|last4=Cristianini|first4=Nello|date=2016-11-08|title=Discovering Periodic Patterns in Historical News|journal=PLOS ONE|volume=11|issue=11|pages=e0165736|doi=10.1371/journal.pone.0165736|issn=1932-6203|pmc=5100883|pmid=27824911}}</ref><ref>Seasonal Fluctuations in Collective Mood Revealed by Wikipedia Searches and Twitter Posts, F Dzogang, T Lansdall-Welfare, N Cristianini – 2016 IEEE International Conference on Data Mining, Workshop on ''Data Mining'' in Human Activity Analysis</ref> As part of the big data revolution, [[gender bias]], [[readability]], content similarity, reader preferences, and even mood have been analyzed based on [[text mining]] methods over millions of documents<ref>{{cite journal | last1 = Flaounas | first1 = I. 

18. 

The term "cultural analytics" (or "culture analytics") is now used by many other researchers, as exemplified by two academic symposiums,<ref>[http://neubauercollegium.uchicago.edu/events/uc/cultural_analytics/ "Cultural Analytics: Computational Approaches to the Study of Culture"], the University of Chicago, May 2015 and [https://sites.google.com/nd.edu/ca2017/home "Cultural Analytics symposium"], Notre Dame University, May 2017.</ref> a four-month long research program at UCLA that brought together 120 leading researchers from university and industry labs,<ref>[http://www.ipam.ucla.edu/programs/long-programs/culture-analytics/ "Culture Analytics program"], Institute for Pure and Applied Mathematics (IPAM), UCLA, March 7–June 10, 2016.</ref> an academic peer-review ''Journal of Cultural Analytics: CA'' established in 2016,<ref>{{Cite web|url=http://culturalanalytics.org/about/|title=Journal of Cultural Analytics: CA|last=|first=|date=|website=|access-date=}}</ref> and academic job listings. 

19. 
Related subfields of digital humanities have emerged like [[software studies]], platform studies, and [[critical code studies]]. 

20. 
(2015) "Mining and discovering biographical information in Difangzhi with a language-model-based approach", ''Proceedings of the 2015 International Conference on Digital Humanities''. 

21. 
Kirsch believes that digital humanities practitioners suffer from problems of being marketers rather than scholars, who attest to the grand capacity of their research more than actually performing new analysis and when they do so, only performing trivial parlor tricks of research. 

22. 
What new theoretical cultural concepts and models are required for studying global digital culture with its new mega-scale, speed, and connectivity. 

23. 
''[http://acrl.ala.org/dh/2014/01/30/digital-humanities-in-the-research-commons-precedents-prospects-3/ Digital Humanities in the Research Commons: Precedents & Prospects]'', Association of College & Research Libraries: dh+lib. 

24. 
In concrete terms, the digital humanities embraces a variety of topics, from curating online collections of primary sources (primarily textual) to the [[data mining]] of large cultural data sets to [[topic modeling]]. 

25. 
Network analysis and data visualization is also used for reflections on the field itself – researchers may produce network maps of social media interactions or infographics from data on digital humanities scholars and projects. 

26. 
Network visualization and analysis published in {{Cite journal | volume = 10| issue = 3| last = Grandjean| first = Martin| title = La connaissance est un réseau| journal =Les Cahiers du Numérique| accessdate = 2014-10-15| date = 2014| pages = 37–54| url = http://www.cairn.info/resume.php?ID_ARTICLE=LCN_103_0037| doi=10.3166/lcn.10.3.37-54}}</ref>

The Slave Societies Digital Archive<ref>{{cite web|url=https://www.vanderbilt.edu/esss/|title=Homepage|website=Slave Societies Digital Archive}}</ref> (formerly Ecclesiastical and Secular Sources for Slave Societies), directed by Jane Landers<ref>{{cite web|url=https://as.vanderbilt.edu/history/bio/jane-landers/|title=People|website=Department of History}}</ref> and hosted at Vanderbilt University, preserves endangered ecclesiastical and secular documents related to Africans and African-descended peoples in slave societies. 

27. 
[File:Tripletsnew2012.png|thumb|right|Narrative network of US Elections 2012<ref name="ReferenceA">Automated analysis of the US presidential elections using Big Data and network analysis; S Sudhahar, GA Veltri, N Cristianini; Big Data & Society 2 (1), 1-28, 2015</ref>]]

== Tools ==
Digital humanities scholars use a variety of digital tools for their research, which may take place in an environment as small as a mobile device or as large as a [[virtual reality]] lab. 

28. 
The Association for Literary and Linguistic Computing (ALLC) and the Association for Computers and the Humanities (ACH) were then founded in 1977 and 1978, respectively.<ref name=":1" />

Soon, there was a need for a standardized protocol for tagging digital texts, and the [[Text Encoding Initiative]] (TEI) was developed.<ref name=":1" /> The TEI project was launched in 1987 and published the first full version of the ''TEI Guidelines'' in May 1994.<ref name=hockney /> TEI helped shape the field of electronic textual scholarship and led to [[Extensible Markup Language]] (XML), which is a tag scheme for digital editing. 

29. 

'''Values'''<ref name="Northwestern University Library"/>
{{Div col begin|colwidth=30em}}
* Critical and theoretical
* Iterative and experimental
* Collaborative and distributed
* Multimodal and performative
* Open and accessible
{{Div col end}}

'''Methods'''<ref name="Northwestern University Library"/>
{{Div col begin|colwidth=30em}}
* Enhanced critical curation
* Augmented editions and fluid textuality
* Scale: the law of large numbers
* Distant/close, macro/micro, surface/depth
* Cultural analytics, aggregation, and data-mining
* Visualization and data design
* Locative investigation and thick mapping
* The animated archive
* Distributed knowledge production and performative access
* Humanities gaming
* Code, software, and platform studies
* Database documentaries
* Repurposable content and remix culture
* Pervasive infrastructure
* Ubiquitous scholarship
{{Div col end}}

In keeping with the value of being open and accessible, many digital humanities projects and journals are [[open access]] and/or under [[Creative Commons]] licensing, showing the field's "commitment to [[open standards]] and [[Open-source model|open source]]."<ref>{{Cite book|title=Collaborative Research in the Digital Humanities|last=Bradley|first=John|publisher=Ashgate|year=2012|isbn=9781409410683|editors=Marilyn Deegan and Willard McCarty (eds.)|location=Farnham and Burlington|pages=11–26 [14]|chapter=No job for techies: Technical contributions to research in digital humanities}}</ref> Open access is designed to enable anyone with an internet-enabled device and internet connection to view a website or read an article without having to pay, as well as share content with the appropriate permissions. 

30. 
* {{Cite journal | volume = 3| issue = 1| last = Grandjean| first = Martin| title = A social network analysis of Twitter: Mapping the digital humanities community| journal = Cogent Arts & Humanities| date = 2016| pages = 1171458| doi=10.1080/23311983.2016.1171458| url = https://hal.archives-ouvertes.fr/hal-01517493/file/A%20social%20network%20analysis%20of%20Twitter%20Mapping%20the%20digital%20humanities%20community.pdf}}
* {{cite journal | last1 = Hancock | first1 = B. 
